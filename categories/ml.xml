<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Amrit's Blog (Posts about ml)</title><link>https://amritpandey23.github.io/</link><description></description><atom:link href="https://amritpandey23.github.io/categories/ml.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="mailto:mail.amritpandey@gmail.com"&gt;Amrit&lt;/a&gt; </copyright><lastBuildDate>Sat, 09 Nov 2024 11:35:49 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>BM25 Algorithm for text retrieval</title><link>https://amritpandey23.github.io/posts/2024/11/bm25-algorithm-for-text-retrieval/</link><dc:creator>Amrit</dc:creator><description>&lt;p&gt;&lt;em&gt;In today’s world, where information is abundant, text retrieval algorithms are crucial for filtering and ranking content based on relevance. Imagine searching for a document among thousands based on a few keywords—how does a search engine know which documents are most relevant? Two widely used methods, &lt;strong&gt;TF-IDF&lt;/strong&gt; (Term Frequency-Inverse Document Frequency) and &lt;strong&gt;BM25&lt;/strong&gt; (Best Matching 25), provide answers to this question. Let's explore how TF-IDF works, why it has limitations, and how BM25 was designed to address these issues.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;How Text Retrieval Works with TF-IDF&lt;/h3&gt;
&lt;p&gt;Before diving into TF-IDF, let’s first understand the basic principles of text retrieval. When you input a search query, the retrieval system tries to find documents containing your keywords and ranks them according to their relevance. The goal is to display documents most relevant to your query at the top.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TF-IDF&lt;/strong&gt; is a fundamental scoring technique used in this process. Its purpose is to identify important words within a document and assign a higher weight to unique terms while down-weighting common terms. The TF-IDF score of a term in a document is calculated as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Term Frequency (TF):&lt;/strong&gt; Counts how often a term appears in a document. A higher frequency means the term is more significant within the document.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inverse Document Frequency (IDF):&lt;/strong&gt; This measures the rarity of a term across the document collection. The idea is to down-weight terms that appear in many documents, as they are less useful for distinguishing between documents. Common words like “the,” “is,” and “and” have low IDF values because they are present in nearly every document, while more unique terms have higher IDF values. The IDF score is calculated as the logarithm of the total number of documents divided by the number of documents containing the term. This helps to emphasize unique words while down-weighting terms that are common across the entire collection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The formula for TF-IDF can be expressed as:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://amritpandey23.github.io/images/bm25-1.png"&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;t&lt;/code&gt; is the term, &lt;code&gt;d&lt;/code&gt; is the document, &lt;code&gt;N&lt;/code&gt; is the total number of documents, and &lt;code&gt;DF(t)&lt;/code&gt; is the number of documents containing the term &lt;code&gt;t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;TF-IDF provides a simple yet effective way to identify and weigh keywords in documents. However, it comes with limitations that can affect retrieval quality, especially in real-world applications.&lt;/p&gt;
&lt;h3&gt;Problems with Traditional TF-IDF&lt;/h3&gt;
&lt;p&gt;Despite its simplicity and effectiveness, TF-IDF has some notable issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Term Saturation Issue&lt;/strong&gt;&lt;br&gt;
   TF-IDF assumes that term frequency contributes linearly to relevance, meaning that a term appearing 10 times is twice as relevant as one appearing 5 times. However, this isn’t always true; a point is reached where adding more occurrences of a term doesn’t significantly increase relevance. TF-IDF doesn’t account for this “saturation effect,” leading to overemphasis on frequently repeated terms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Document Length Bias&lt;/strong&gt;&lt;br&gt;
   Longer documents are likely to contain more occurrences of search terms, simply because they have more content. TF-IDF tends to favor longer documents, even if they aren’t necessarily more relevant. This length bias means shorter, more focused documents may be ranked lower, even when they may be more relevant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of Parameter Control&lt;/strong&gt;&lt;br&gt;
   TF-IDF is relatively rigid, with limited customization for different types of documents or retrieval scenarios. The scoring is solely based on term frequency and inverse document frequency, with no additional parameters to adjust for nuances like document length or term importance saturation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These limitations can cause TF-IDF to yield suboptimal results in real-world scenarios. This is where BM25, a more advanced probabilistic model, comes into play.&lt;/p&gt;
&lt;h3&gt;How BM25 Improves Text Retrieval&lt;/h3&gt;
&lt;p&gt;BM25, or Best Matching 25, was developed as part of the Okapi BM model family to address these very issues. BM25 is a scoring function that builds on TF-IDF but introduces additional controls, making it more flexible and effective in handling diverse document collections.&lt;/p&gt;
&lt;p&gt;Here’s how BM25 addresses the limitations of TF-IDF:&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;Non-Linear Term Frequency Scaling&lt;/strong&gt;&lt;br&gt;
 BM25 introduces a concept called &lt;em&gt;term saturation&lt;/em&gt;, where the relevance score increases with term frequency but at a diminishing rate. In other words, if a term appears five times in a document, the relevance increase is substantial, but after a certain threshold, additional occurrences contribute less and less to the score. This prevents documents from being unfairly ranked higher solely because of excessively repeated terms. BM25 achieves this using the parameter &lt;code&gt;k1&lt;/code&gt;, which controls the term frequency’s impact. The formula is:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://amritpandey23.github.io/images/bm25-2.png"&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;k1&lt;/code&gt; can be adjusted based on the dataset to control how quickly the term frequency impact diminishes.&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;Document Length Normalization&lt;/strong&gt;&lt;br&gt;
 Unlike TF-IDF, BM25 considers document length when calculating relevance. It uses a parameter &lt;code&gt;b&lt;/code&gt; to normalize for length, ensuring that longer documents are not unfairly favored over shorter ones. This length normalization helps prevent long documents from automatically receiving higher scores and allows shorter, more focused documents to be fairly ranked.&lt;/p&gt;
&lt;p&gt;The formula for BM25 length normalization is:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://amritpandey23.github.io/images/bm25-3.png"&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;L_d&lt;/code&gt; is the document length, and &lt;code&gt;L_avg&lt;/code&gt; is the average document length across the collection. The parameter &lt;code&gt;b&lt;/code&gt; (typically between 0 and 1) controls the degree of length normalization.&lt;/p&gt;
&lt;p&gt;3.&lt;strong&gt;Tunable Parameters for Greater Flexibility&lt;/strong&gt;&lt;br&gt;
 BM25’s parameters &lt;code&gt;k1&lt;/code&gt; (for term frequency scaling) and &lt;code&gt;b&lt;/code&gt; (for length normalization) allow it to be customized for different types of datasets and retrieval needs. This tunability makes BM25 adaptable for various scenarios, from large document collections to shorter, focused datasets.&lt;/p&gt;
&lt;h3&gt;Why BM25 Is Superior for Modern Text Retrieval&lt;/h3&gt;
&lt;p&gt;BM25 has become the default scoring model in many search engines and information retrieval systems, including Elasticsearch and Apache Lucene, thanks to its practical enhancements over TF-IDF. By addressing the limitations of term saturation, document length bias, and lack of flexibility, BM25 produces more accurate and reliable rankings, making it highly effective for real-world applications.&lt;/p&gt;
&lt;p&gt;The differences may seem subtle, but in high-stakes search scenarios, these optimizations make a big difference. BM25’s ability to fine-tune term impact and document length ensures users receive relevant results without the pitfalls that plagued TF-IDF.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;While TF-IDF laid the groundwork for text retrieval, BM25 built upon it to solve key issues, enhancing search relevance and performance. Today, BM25 remains a go-to algorithm for ranking documents effectively, proving its enduring value in the field of Information Retrieval.&lt;/p&gt;</description><category>ml</category><guid>https://amritpandey23.github.io/posts/2024/11/bm25-algorithm-for-text-retrieval/</guid><pubDate>Sat, 09 Nov 2024 10:48:16 GMT</pubDate></item></channel></rss>