<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Key insights from creating a practical text search app with vector embeddings and RAG, without frameworks or external APIs.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Insights from Building an Embeddings and Retrieval-Augmented Generation App | Amrit's Blog</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../../rss.xml">
<link rel="canonical" href="https://amritpandey23.github.io/posts/2024/11/insights-from-embeddings-and-retrieval-augmented-generation/">
<link rel="icon" href="../../../../images/favicon.ico" sizes="16x16">
<link rel="icon" href="../../../../images/android-chrome-192x192.png" sizes="192x192">
<!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous" referrerpolicy="no-referrer">
<meta name="author" content="Amrit">
<link rel="prev" href="../bm25-algorithm-for-text-retrieval/" title="BM25 Algorithm for text retrieval" type="text/html">
<meta property="og:site_name" content="Amrit's Blog">
<meta property="og:title" content="Insights from Building an Embeddings and Retrieval-Augmented Generatio">
<meta property="og:url" content="https://amritpandey23.github.io/posts/2024/11/insights-from-embeddings-and-retrieval-augmented-generation/">
<meta property="og:description" content="Key insights from creating a practical text search app with vector embeddings and RAG, without frameworks or external APIs.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-11-10T00:00:00+05:30">
<meta property="article:tag" content="genai">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-light
bg-light
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../../../../">

            <span id="blog-title">Amrit's Blog</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../../../archive.html" class="nav-link">Blogs</a>
                </li>
<li class="nav-item">
<a href="../../../../pages/about/" class="nav-link">About</a>
            </li>
<li class="nav-item dropdown">
<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Meta</a>
            <div class="dropdown-menu">
                    <a href="../../../../categories/" class="dropdown-item">Tags</a>
                    <a href="../../../../pages/knowledge-base/" class="dropdown-item">Knowledge Base</a>
            </div>

                
            </li>
</ul>
<!-- DuckDuckGo custom search --><form method="get" id="search" action="https://duckduckgo.com/" class="form-inline" role="search">
<input type="hidden" name="sites" value="https://amritpandey23.github.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><div class="form-group mr-2" style="margin-bottom: 0">
    <input class="form-control" type="search" name="q" maxlength="255" placeholder="Search...">
</div>
<button type="submit" class="btn btn-primary">
<i class="fa-solid fa-magnifying-glass"></i>
</button>
</form>
<!-- End of custom search -->


            <ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Insights from Building an Embeddings and Retrieval-Augmented Generation App</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Amrit
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2024-11-10T00:00:00+05:30" itemprop="datePublished" title="11-2024">11-2024</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p><em>In this post, I‚Äôll share key insights and findings from building a practical text search application without using frameworks like LangChain or external APIs. I've also extended the app‚Äôs functionality to support Retrieval-Augmented Generation (RAG) capabilities using the Gemini Flash 1.5B model.</em></p>
<hr>
<h3>Understanding Text Retrieval üìö</h3>
<p>Text retrieval is the science of extracting relevant information from a dataset in response to a query. This challenge predates modern digital systems‚Äîhistorically, information was retrieved through manually annotated documents.</p>
<p>After World War II, early text retrieval solutions addressed the growing need to index large scientific publications. These systems evolved into what we now know as "search engines." Early systems relied on metadata, like human annotations, to solve specific retrieval problems, distinct from today‚Äôs web-based search engines.</p>
<p>In the following decades, methods for automated text retrieval emerged, such as term-frequency and inverse-document-frequency (TF-IDF) algorithms, which match the frequency of terms in the query and corpus. Today, a variant known as <code>BM25</code> remains widely used in text retrieval.</p>
<h3>Vectors and Embeddings ‚öõÔ∏è</h3>
<p>A vector is a mathematical representation, essentially a set of numbers, such as <code>[1, 2, 3]</code>. Assigning meaning to these numbers makes them useful. For example, in X, Y, Z dimensions, <code>[1, 2, 3]</code> might represent an object‚Äôs location in 3D space.</p>
<p>In text retrieval and natural language processing (NLP), vectors represent data numerically, enabling computers to semantically compare sentence meanings. Machine learning models trained on large datasets can convert text into numerical vectors, known as <em>embeddings</em>.</p>
<p>For instance, the sentence "<em>Hello, My Name is Amrit</em>" could be represented as an embedding vector like <code>[-0.23, 0.78, 1.22, -0.034 ...]</code>. Below, you‚Äôll see an example of a text embedding vector of size 512:</p>
<p><img alt="" src="../../../../images/insomnia_output.png"></p>
<h3>What is Retrieval-Augmented Generation (RAG)? üñ®Ô∏è</h3>
<p><img alt="" src="../../../../images/rag_diagram.jpg"></p>
<p>Retrieval-Augmented Generation (RAG) uses three key elements to generate relevant responses with Large Language Models (LLMs) -- <em>Information Retrieval</em>, <em>Knowledge-Augmentation</em> and <em>Text Generation</em>.</p>
<p>Here‚Äôs a breakdown of each component:</p>
<h5>Information Retrieval</h5>
<p>As discussed in the <a href="#understanding-text-retrieval">text retrieval section</a>, this involves extracting relevant information from a corpus. Our application is designed to retrieve accurate, relevant data to prevent RAG from returning irrelevant results.</p>
<p>The IR systems can use algorithms that depends on exact query matching in order to retrieve relevant text data. There are many ranking and relevance algorithms which performs to fetch relevant data from the information system. The query and ranking algorithms are then combined with similarity matching algorithms from the realm of Natural Language Processing (NLP) in order to furture tune the search results.</p>
<h5>Knowledge Augmentation</h5>
<p>Augmentation provides the model with context or additional information to improve responses. Known as "Knowledge Augmentation," this technique supplements LLMs with information they may lack. However, each LLM has a token limit, so our retrieval system must fetch concise, relevant information so that the text-generation process may provide user with accurate response.</p>
<h5>Text Generation</h5>
<p>Once retrieval and augmentation are complete, the LLM generates a response based on the relevant, augmented data.</p>
<p>RAG is critical for generating responses on topics an LLM isn‚Äôt familiar with, serving as an alternative to fine-tuning, which can be time-consuming and costly.</p>
<h3>Building My Text Embedding and RAG Application üöÄ</h3>
<p>There's a common adage in engineering: <em>never reinvent the wheel</em>. I don‚Äôt fully subscribe to this. If we don‚Äôt understand how the wheel was originally invented, how can we hope to design something as complex as a car? Building from scratch is essential for understanding and innovation.</p>
<p>For this project, several frameworks could have provided out-of-the-box utilities, such as LangChain, Cohere's semantic search library, or OpenAI‚Äôs text embedding model. However, using these would have only taught me to work with their APIs, not the underlying principles.</p>
<p><img alt="" src="../../../../images/learnings_from_text_embeddings_RAG.jpg"></p>
<p>I started by designing a vector database from scratch, using <strong>SQLite</strong> with the <code>sqlite-vec</code> extension to convert it into a vector database. I created wrappers around SQLite to abstract low-level operations, keeping the implementation clean and manageable.</p>
<p>For text embeddings, I initially used <strong>TensorFlow‚Äôs Wiki-Words</strong> model but later switched to <strong>Google‚Äôs Universal Sentence Encoder</strong>, which generates embeddings of size <strong>512</strong>, offering improved accuracy.</p>
<p>The LLM, <strong>Gemini Flash 1.5B</strong>, is my only external dependency for generating responses. I‚Äôm exploring on-device models as this is a field I‚Äôd like to delve deeper into.</p>
<p>All these are tied well together with a simple <strong>Flask</strong> application as a backend server.</p>
<p>The front-end is a <strong>React</strong> application that communicates with the backend via CORS, with a sleek, simple UI and customizable client-side settings.</p>
<p>This is a common <strong>light-weight architecture</strong> I follow for creating all of my personal apps, you can read more about the architecture <a href="https://amritpandey23.github.io/posts/2024/09/a-simple-and-scalable-architecture-for-your-next-to-do-app/">here</a>.</p>
<h5>Search Functionality</h5>
<p>The search process works as follows:</p>
<ol>
<li>The user‚Äôs query is converted into an embedding, and the system retrieves the nearest neighbors from the vector database.</li>
<li>The results are displayed in ascending order of distance from the query embedding, showing the closest matches first.</li>
</ol>
<p><img alt="" src="../../../../images/search_results.png"></p>
<h5>RAG for Question-Answering</h5>
<p>For question-answering:</p>
<ol>
<li>The user‚Äôs question is converted into an embedding, and the system retrieves relevant embeddings from the vector database.</li>
<li>The LLM then generates a response based on this context.</li>
</ol>
<p><img alt="" src="../../../../images/ask_results.png"></p>
<h5>Data Operations</h5>
<p>The Data Operations page allows users to add new data to the database, expanding available data for search and responses. Users enter text data, which is converted into embeddings and stored in the vector database.</p>
<p><img alt="" src="../../../../images/data_operations.png"></p>
<h4>Key Technical Details ü§ì</h4>
<p>This section dives into some technical aspects of the app, presented in a Q&amp;A format for those interested in the finer points.</p>
<h5>How Do Vector Databases Differ from Traditional Databases?</h5>
<p>Traditional databases store structured data in various formats. Vector databases, by contrast, are specialized for storing multi-dimensional vectors. They don‚Äôt differentiate between data types like images, text, or audio, as all unstructured data is stored in vectors.</p>
<p>Vector databases support operations like cosine similarity, distance, and dot product, unlike CRUD operations typical in traditional databases.</p>
<p>The <code>text-embeddings</code> table below shows how the database stores text embeddings, each associated with a row ID for retrieval. The database also includes <code>chunks</code> and <code>rowId</code> tables for breaking down large texts, though chunking isn‚Äôt implemented in this app.</p>
<p><img alt="" src="../../../../images/vector_database_structure.png"></p>
<h5>Can Embeddings Be Converted Back to Text?</h5>
<p>Embeddings don‚Äôt store word-to-number mappings, making it impossible to retrieve exact text from them. Techniques like similarity search (used in this app) can retrieve text with similar meanings, but converting embeddings back to the original text is mathematically impossible and computationally challenging.</p>
<h5>How Does Text Length Affect Embeddings?</h5>
<p>Embedding length is designed to capture a text‚Äôs meaning. Shorter texts provide more focused embeddings, while longer texts can lose detail as the model compresses information.</p>
<p>To address this, <em>chunking</em> breaks down longer texts into smaller, meaningful pieces, storing embeddings for each chunk. This improves search accuracy and can add metadata for better context.</p>
<p>I learned about this technique but didn't implemented it yet, something to look for in the futureüòâ.</p>
<p><img alt="" src="../../../../images/chunking_diagram.jpg"></p>
<h5>How Does the Search Work?</h5>
<p>The app uses a K-Nearest Neighbor (KNN) algorithm from <code>sqlite-vec</code> to find the closest matches for a query embedding. Here‚Äôs a simplified SQL example:</p>
<div class="code"><pre class="code literal-block"><span class="k">SELECT</span>
<span class="w">    </span><span class="n">rowid</span><span class="p">,</span>
<span class="w">    </span><span class="n">distance</span>
<span class="k">FROM</span>
<span class="w">    </span><span class="n">text_embeddings</span>
<span class="k">WHERE</span>
<span class="w">    </span><span class="n">sample_embedding</span>
<span class="k">MATCH</span>
<span class="w">    </span><span class="o">?</span>
<span class="k">AND</span>
<span class="w">    </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="n">k</span><span class="err">}</span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span>
<span class="w">    </span><span class="n">distance</span>
</pre></div>

<p>The KNN algorithm calculates the distance between vectors in a multi-dimensional space (512 in this case) to find the nearest matches. Other algorithms, like cosine similarity, are also common for comparing text similarity in information retrieval. Given below you can see the search performed on the existing corpora of text shown in ascending order of the distance from the query text.</p>
<p><img alt="" src="../../../../images/insomnia_output_2.png"></p>
<h5>How Does the Prompt Look?</h5>
<p>The prompt used in the app to generate responses is available on <a href="https://aistudio.google.com/app/prompts">Google AI Studio</a>.</p>
<h3>Source Code ‚≠ê</h3>
<p>The full source code, including front-end and back-end, is available on GitHub: <a href="https://github.com/amritpandey23/Sanjaya">Sanjaya</a>.</p>
<p>The embedding models used include:</p>
<ul>
<li><a href="https://www.kaggle.com/models/google/universal-sentence-encoder">Google Universal Sentence Encoder on Kaggle</a></li>
<li><a href="https://www.kaggle.com/models/google/wiki-words">Google Wiki-Words model on Kaggle</a></li>
</ul>
<p>For instructions on setting up a vector database with SQLite, see the <a href="https://github.com/asg017/sqlite-vec">sqlite-vec extension</a>. The LLM model Gemini is available on <a href="https://aistudio.google.com/prompts/new_chat">Google AI Studio</a>.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../../../categories/genai/" rel="tag">genai</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../bm25-algorithm-for-text-retrieval/" rel="prev" title="BM25 Algorithm for text retrieval">Previous post</a>
            </li>
        </ul></nav></aside></article><!--End of body content--><footer id="footer"></footer>
</div>
</div>


        <script src="../../../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script><footer class="py-4 footer-background"><div class="container text-center">
      <p class="mb-2">Connect with me</p>
      <div class="d-flex justify-content-center mb-3">
        <a href="https://www.youtube.com/@AmritPandey" target="_blank" class="social-link mx-3" aria-label="YouTube">
          <i class="fab fa-youtube fa-2x"></i> YouTube
        </a>
        <a href="https://www.linkedin.com/in/amritpandey23/" target="_blank" class="social-link mx-3" aria-label="LinkedIn">
          <i class="fab fa-linkedin fa-2x"></i> Linkedin
        </a>
        <a href="https://github.com/amritpandey23" target="_blank" class="social-link mx-3" aria-label="GitHub">
          <i class="fab fa-github fa-2x"></i> GitHub
        </a>
      </div>
      <p class="small mb-0">Contents ¬© 2024 Amrit Pandey. All rights reserved.</p>
    </div>
  </footer>
</body>
</html>
